{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educated-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File processing\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data display \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import linalg as LAtorch\n",
    "from numpy import linalg as LAnumpy\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch_geometric.data import Data, InMemoryDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac13c5",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465431e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 50\n",
    "BATCH_SIZE = 10\n",
    "NB_BINS = 202\n",
    "EMBEDDING_SIZE = 3 # Euclidean 3D space\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 0\n",
    "LAMBDA_BIOLOGICAL = 1\n",
    "LAMBDA_KABSCH = 1\n",
    "TRAIN_DATASET_SIZE = 800\n",
    "TEST_DATASET_SIZE = 200\n",
    "NOISE_VARIANCE = 0\n",
    "\n",
    "# Trussart analysis dataset constants\n",
    "TRUSSART_HIC_PATH = '../../../../../../experiments/data/trussart/hic_matrices/150_TADlike_alpha_150_set0.mat'\n",
    "TRUSSART_STRUCTURES_PATH = '../../../../../../experiments/data/trussart/structure_matrices/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc7277",
   "metadata": {},
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26605809",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e449bb",
   "metadata": {},
   "source": [
    "# Trussart analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412f446",
   "metadata": {},
   "source": [
    "## Hic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "trussart_hic = np.loadtxt(TRUSSART_HIC_PATH, dtype='f', delimiter='\\t')\n",
    "scaler = MinMaxScaler()\n",
    "trussart_hic = scaler.fit_transform(trussart_hic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b876b",
   "metadata": {},
   "source": [
    "## Structure matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43eefd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trussart_structures = []\n",
    "\n",
    "file_list = os.listdir(TRUSSART_STRUCTURES_PATH)\n",
    "file_list = filter(lambda f: f.endswith('.xyz'), file_list)\n",
    "\n",
    "for file_name in file_list:\n",
    "    current_trussart_structure = np.loadtxt(TRUSSART_STRUCTURES_PATH + file_name, dtype='f', delimiter='\\t')\n",
    "    current_trussart_structure = current_trussart_structure[:,1:]\n",
    "    trussart_structures.append(current_trussart_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-gnome",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab last 4 digits of the file txt name:\n",
    "def last_4digits(x):\n",
    "    return(x[-8:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867069cd",
   "metadata": {},
   "source": [
    "### Hic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957f0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_hics = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/train/hic_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_hic = np.loadtxt('../../../../data/synthetic_biological/train/hic_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_hics.append(current_train_transfer_learning_hic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3704005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_hics = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/test/hic_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_hic = np.loadtxt('../../../../data/synthetic_biological/test/hic_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_hics.append(current_test_transfer_learning_hic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef01a0",
   "metadata": {},
   "source": [
    "### Structure matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c29a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_structures = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/train/structure_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_structure = \\\n",
    "        np.loadtxt('../../../../data/synthetic_biological/train/structure_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_structures.append(current_train_transfer_learning_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a41608",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_structures = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/test/structure_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_structure = \\\n",
    "        np.loadtxt('../../../../data/synthetic_biological/test/structure_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_structures.append(current_test_transfer_learning_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24328a71",
   "metadata": {},
   "source": [
    "### Distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182f8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_distances = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/train/distance_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_distance = \\\n",
    "            np.loadtxt('../../../../data/synthetic_biological/train/distance_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_distances.append(current_train_transfer_learning_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeebfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_distances = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/test/distance_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_distance = \\\n",
    "            np.loadtxt('../../../../data/synthetic_biological/test/distance_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_distances.append(current_test_transfer_learning_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0a949",
   "metadata": {},
   "source": [
    "### Final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cfddc",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ecf83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulation-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(VanillaDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if is_training:\n",
    "            return ['non_ae_synthetic_biological_linear_train_data.txt']\n",
    "        else:\n",
    "            return ['non_ae_synthetic_biological_linear_test_data.txt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        if is_training:\n",
    "            dataset_size = TRAIN_DATASET_SIZE\n",
    "        else:\n",
    "            dataset_size = TEST_DATASET_SIZE\n",
    "        \n",
    "        for i in tqdm(range(dataset_size)):\n",
    "            \n",
    "            if is_training:\n",
    "                transfer_learning_hic = train_transfer_learning_hics[i]\n",
    "                transfer_learning_structure = train_transfer_learning_structures[i]\n",
    "                transfer_learning_distance_matrix = train_transfer_learning_distances[i]\n",
    "            else:\n",
    "                transfer_learning_hic = test_transfer_learning_hics[i]\n",
    "                transfer_learning_structure = test_transfer_learning_structures[i]\n",
    "                transfer_learning_distance_matrix = test_transfer_learning_distances[i]\n",
    "               \n",
    "            hic_matrix = torch.FloatTensor(transfer_learning_hic)\n",
    "            structure_matrix = torch.FloatTensor(transfer_learning_structure)\n",
    "            distance_matrix = torch.FloatTensor(transfer_learning_distance_matrix)\n",
    "\n",
    "            data = Data(hic_matrix=hic_matrix, structure_matrix=structure_matrix, distance_matrix=distance_matrix)\n",
    "            data_list.append(data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incredible-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 40597.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VanillaDataset('../')\n",
    "train_dataset = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "considered-wealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(train_dataset)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b9ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6adbb",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a45875f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd917ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 47926.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = VanillaDataset('../')\n",
    "test_dataset = test_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d6319b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(test_dataset)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ede05c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-invite",
   "metadata": {},
   "source": [
    "# Synthetic Biological Network: Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e7a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear_encoder_layer_x = torch.nn.Linear(NB_BINS, NB_BINS)\n",
    "        \n",
    "        self.linear_decoder_layer_z = torch.nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        \n",
    "    def forward(self, x, is_training=False):\n",
    "        \n",
    "        x = torch.reshape(x, (BATCH_SIZE, NB_BINS, NB_BINS))\n",
    "        if is_training:\n",
    "            x = x + (NOISE_VARIANCE**0.5)*torch.randn(BATCH_SIZE, NB_BINS, NB_BINS)\n",
    "        \n",
    "        x = self.linear_encoder_layer_x(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        z, _ = synthetic_random_model(x)\n",
    "        \n",
    "        # Deal with z\n",
    "        z = self.linear_decoder_layer_z(z)\n",
    "        z = F.relu(z)\n",
    "        z = centralize_and_normalize_torch(z)\n",
    "        \n",
    "        w = torch.cdist(z, z, p=2)\n",
    "        \n",
    "        return z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771473b",
   "metadata": {},
   "source": [
    "# Structure analysis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74381471",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10db24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_torch(z):\n",
    "    return z - torch.repeat_interleave(torch.reshape(torch.mean(z, axis=1), (-1,1,EMBEDDING_SIZE)), NB_BINS, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2056afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_torch(z):\n",
    "    \n",
    "    norms = LAtorch.norm(z, 2, dim=2)\n",
    "    max_norms, _ = torch.max(norms, axis=1)\n",
    "    max_norms = torch.reshape(max_norms, (BATCH_SIZE,1,1))\n",
    "    max_norms = torch.repeat_interleave(max_norms, NB_BINS, dim=1)\n",
    "    max_norms = torch.repeat_interleave(max_norms, EMBEDDING_SIZE, dim=2)\n",
    "    max_norms[max_norms == 0] = 1\n",
    "    \n",
    "    return z / max_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba06874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_and_normalize_torch(z):\n",
    "    \n",
    "    # Translate\n",
    "    z = centralize_torch(z)\n",
    "    \n",
    "    # Scale\n",
    "    z = normalize_torch(z)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e764b0",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30307ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_numpy(z):\n",
    "    return z - np.mean(z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6688fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numpy(z):\n",
    "    \n",
    "    norm = LAnumpy.norm(z, 2, axis=1)\n",
    "    max_norm = np.max(norm, axis=0)\n",
    "    if max_norm == 0:\n",
    "        max_norm = 1\n",
    "    \n",
    "    return z / max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87f22cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_and_normalize_numpy(z):\n",
    "    \n",
    "    # Translate\n",
    "    z = centralize_numpy(z)\n",
    "    \n",
    "    # Scale\n",
    "    z = normalize_numpy(z)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e885c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_superimposition_numpy(pred_structure, true_structure):\n",
    "    \n",
    "    # Centralize and normalize to unit ball\n",
    "    pred_structure_unit_ball = centralize_and_normalize_numpy(pred_structure)\n",
    "    true_structure_unit_ball = centralize_and_normalize_numpy(true_structure)\n",
    "    \n",
    "    # Rotation (solution for the constrained orthogonal Procrustes problem, subject to det(R) = 1)\n",
    "    m = np.matmul(np.transpose(true_structure_unit_ball), pred_structure_unit_ball)\n",
    "    u, s, vh = np.linalg.svd(m)\n",
    "    \n",
    "    d = np.sign(np.linalg.det(np.matmul(u, vh)))\n",
    "    a = np.eye(EMBEDDING_SIZE)\n",
    "    a[-1,-1] = d\n",
    "    \n",
    "    r = np.matmul(np.matmul(u, a), vh)\n",
    "    \n",
    "    pred_structure_unit_ball = np.transpose(np.matmul(r, np.transpose(pred_structure_unit_ball)))\n",
    "    \n",
    "    return pred_structure_unit_ball, true_structure_unit_ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18470850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_distance_numpy(pred_structure, true_structure):\n",
    "    \n",
    "    pred_structure_unit_ball, true_structure_unit_ball = kabsch_superimposition_numpy(pred_structure, true_structure)\n",
    "    \n",
    "    # Structure comparison\n",
    "    d = np.mean(np.sum(np.square(pred_structure_unit_ball - true_structure_unit_ball), axis=1))\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed946e0",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac233131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trussart_test_kabsch_loss():\n",
    "    \n",
    "    torch_trussart_hic = torch.FloatTensor(trussart_hic)\n",
    "    torch_trussart_hic = torch.reshape(torch_trussart_hic, (1, NB_BINS, NB_BINS))\n",
    "    torch_trussart_hic = torch.repeat_interleave(torch_trussart_hic, BATCH_SIZE, 0)\n",
    "    \n",
    "    trussart_pred_structure, _ = model(torch_trussart_hic)\n",
    "    trussart_pred_structure = trussart_pred_structure.detach().numpy()[0]\n",
    "    \n",
    "    kabsch_distances = []\n",
    "    for true_structure in trussart_structures:\n",
    "        kabsch_distances.append(kabsch_distance_numpy(trussart_pred_structure, true_structure))\n",
    "        \n",
    "    return np.mean(kabsch_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8119ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biological_loss_fct(pred_structure, true_structure, pred_distance, true_distance):\n",
    "    \n",
    "    ####### Pairwise distances loss ########\n",
    "    \n",
    "    between_bin_distance = \\\n",
    "        torch.diagonal(pred_distance.reshape((BATCH_SIZE, NB_BINS, NB_BINS)), offset=1, dim1=1, dim2=2)\n",
    "    between_bin_distance_loss = torch.var(between_bin_distance)\n",
    "    \n",
    "    ######### Pairwise angles loss ##########\n",
    "    \n",
    "    pred_structure_vectors = (pred_structure - torch.roll(pred_structure, 1, dims=1))[:, 1:, :]\n",
    "    pred_structure_dot_products = \\\n",
    "        torch.matmul(pred_structure_vectors, torch.transpose(pred_structure_vectors, dim0=1, dim1=2))\n",
    "    pairwise_angles_loss = \\\n",
    "        torch.where(pred_structure_dot_products < 0, torch.ones((BATCH_SIZE, NB_BINS-1, NB_BINS-1))*0.1, \\\n",
    "                        torch.zeros((BATCH_SIZE, NB_BINS-1, NB_BINS-1)))\n",
    "    pairwise_angles_loss = torch.mean(pairwise_angles_loss)\n",
    "    \n",
    "    return between_bin_distance_loss + pairwise_angles_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_loss_fct(pred_structure, true_structure):\n",
    "    \n",
    "    # NOTE: the two input structures should already be centralized and normalized\n",
    "    \n",
    "    m = torch.matmul(torch.transpose(true_structure, 1, 2), pred_structure)\n",
    "    u, s, vh = torch.linalg.svd(m)\n",
    "    \n",
    "    d = torch.sign(torch.linalg.det(torch.matmul(u, vh)))\n",
    "    a = torch.eye(EMBEDDING_SIZE).reshape((1, EMBEDDING_SIZE, EMBEDDING_SIZE)).repeat_interleave(BATCH_SIZE, dim=0)\n",
    "    a[:,-1,-1] = d\n",
    "    \n",
    "    r = torch.matmul(torch.matmul(u, a), vh)\n",
    "    \n",
    "    pred_structure = torch.transpose(torch.matmul(r, torch.transpose(pred_structure, 1, 2)), 1, 2)\n",
    "    \n",
    "    return torch.mean(torch.sum(torch.square(pred_structure - true_structure), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74f3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_loss_fct = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-prerequisite",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "contemporary-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52b37ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([matplotlib.lines.Line2D([0], [0], color=\"c\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"b\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"k\", lw=4)], \n",
    "               ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "artificial-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_structure, pred_distance = model(data.hic_matrix, is_training=True)\n",
    "        \n",
    "        true_hic = data.hic_matrix.to(device)\n",
    "        \n",
    "        true_structure = data.structure_matrix.to(device)\n",
    "        true_structure = torch.reshape(true_structure, (BATCH_SIZE, NB_BINS, EMBEDDING_SIZE))\n",
    "        \n",
    "        pred_distance = torch.reshape(pred_distance, (BATCH_SIZE*NB_BINS, NB_BINS))\n",
    "        true_distance = data.distance_matrix.to(device)\n",
    "        \n",
    "        # Biological loss\n",
    "        biological_loss = biological_loss_fct(pred_structure, true_structure, pred_distance, true_distance)\n",
    "        \n",
    "        # Kabsch loss\n",
    "        kabsch_loss = kabsch_loss_fct(pred_structure, true_structure)\n",
    "        \n",
    "        # Distance loss \n",
    "        distance_loss = distance_loss_fct(pred_distance, true_distance)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = LAMBDA_BIOLOGICAL * biological_loss + LAMBDA_KABSCH * kabsch_loss + distance_loss\n",
    "        \n",
    "#         with torch.autograd.detect_anomaly():\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        \n",
    "        # Plot grad flow\n",
    "#         plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "operating-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    true_hics = []\n",
    "    \n",
    "    pred_structures = []\n",
    "    true_structures = []\n",
    "    \n",
    "    pred_distances = []\n",
    "    true_distances = []\n",
    "    \n",
    "    kabsch_losses = []\n",
    "    distance_losses = []\n",
    "    biological_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            \n",
    "            pred_structure, pred_distance = model(data.hic_matrix)\n",
    "            \n",
    "            pred_structure = pred_structure.detach().cpu()\n",
    "            pred_distance = pred_distance.detach().cpu()\n",
    "            \n",
    "            pred_distance = torch.reshape(pred_distance, (BATCH_SIZE*NB_BINS, NB_BINS))\n",
    "            \n",
    "            true_hic = data.hic_matrix.detach().cpu()\n",
    "            true_structure = data.structure_matrix.detach().cpu()\n",
    "            true_distance = data.distance_matrix.detach().cpu()\n",
    "            \n",
    "            true_structure = torch.reshape(true_structure, (BATCH_SIZE, NB_BINS, EMBEDDING_SIZE))\n",
    "            \n",
    "            # Biological loss\n",
    "            biological_loss = \\\n",
    "                biological_loss_fct(pred_structure, true_structure, pred_distance, true_distance).numpy()\n",
    "            biological_losses.append(biological_loss)\n",
    "            \n",
    "            # Kabsch loss\n",
    "            kabsch_loss = kabsch_loss_fct(pred_structure, true_structure).numpy()\n",
    "            kabsch_losses.append(kabsch_loss)\n",
    "            \n",
    "            # Distance loss\n",
    "            distance_loss = distance_loss_fct(pred_distance, true_distance).numpy()\n",
    "            distance_losses.append(distance_loss)\n",
    "            \n",
    "            # To numpy\n",
    "            true_hic = true_hic.numpy()\n",
    "            \n",
    "            pred_structure = pred_structure.numpy()\n",
    "            true_structure = true_structure.numpy()\n",
    "            \n",
    "            pred_distance = pred_distance.numpy()\n",
    "            true_distance = true_distance.numpy()\n",
    "            \n",
    "            # Store results\n",
    "            true_hics.append(true_hic)\n",
    "            \n",
    "            pred_structures.append(pred_structure)\n",
    "            true_structures.append(true_structure)\n",
    "            \n",
    "            pred_distances.append(pred_distance)\n",
    "            true_distances.append(true_distance)\n",
    "    \n",
    "    # Format results\n",
    "    true_hics = np.vstack(true_hics)\n",
    "    \n",
    "    pred_structures = np.vstack(pred_structures)\n",
    "    true_structures = np.vstack(true_structures)\n",
    "    \n",
    "    pred_distances = np.vstack(pred_distances)\n",
    "    true_distances = np.vstack(true_distances)\n",
    "    \n",
    "    # Compute mean losses\n",
    "    mean_biological_loss = np.mean(np.asarray(biological_loss).flatten())\n",
    "    mean_kabsch_loss = np.mean(np.asarray(kabsch_losses).flatten())\n",
    "    mean_distance_loss = np.mean(np.asarray(distance_losses).flatten())\n",
    "    \n",
    "    return mean_biological_loss, mean_kabsch_loss, mean_distance_loss, true_hics, \\\n",
    "            pred_structures, true_structures, pred_distances, true_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sacred-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 001, Tr B: 0.0509, Tr K: 0.0573, Tr D: 0.0276, Te B: 0.0506, Te K: 0.0534, Te D: 0.0258, Trus: 0.1604\n",
      "E: 002, Tr B: 0.0503, Tr K: 0.0547, Tr D: 0.0249, Te B: 0.0500, Te K: 0.0509, Te D: 0.0229, Trus: 0.1582\n",
      "E: 003, Tr B: 0.0501, Tr K: 0.0516, Tr D: 0.0226, Te B: 0.0495, Te K: 0.0471, Te D: 0.0202, Trus: 0.1677\n",
      "E: 004, Tr B: 0.0500, Tr K: 0.0505, Tr D: 0.0215, Te B: 0.0495, Te K: 0.0462, Te D: 0.0193, Trus: 0.1693\n",
      "E: 005, Tr B: 0.0499, Tr K: 0.0491, Tr D: 0.0214, Te B: 0.0494, Te K: 0.0448, Te D: 0.0192, Trus: 0.1716\n",
      "E: 006, Tr B: 0.0499, Tr K: 0.0487, Tr D: 0.0212, Te B: 0.0493, Te K: 0.0443, Te D: 0.0190, Trus: 0.1683\n",
      "E: 007, Tr B: 0.0497, Tr K: 0.0485, Tr D: 0.0209, Te B: 0.0492, Te K: 0.0441, Te D: 0.0188, Trus: 0.1785\n",
      "E: 008, Tr B: 0.0495, Tr K: 0.0472, Tr D: 0.0205, Te B: 0.0492, Te K: 0.0429, Te D: 0.0185, Trus: 0.1718\n",
      "E: 009, Tr B: 0.0497, Tr K: 0.0471, Tr D: 0.0200, Te B: 0.0492, Te K: 0.0428, Te D: 0.0180, Trus: 0.1759\n",
      "E: 010, Tr B: 0.0497, Tr K: 0.0468, Tr D: 0.0199, Te B: 0.0495, Te K: 0.0424, Te D: 0.0178, Trus: 0.1852\n",
      "E: 011, Tr B: 0.0495, Tr K: 0.0463, Tr D: 0.0201, Te B: 0.0492, Te K: 0.0422, Te D: 0.0180, Trus: 0.1904\n",
      "E: 012, Tr B: 0.0494, Tr K: 0.0459, Tr D: 0.0199, Te B: 0.0490, Te K: 0.0418, Te D: 0.0178, Trus: 0.1843\n",
      "E: 013, Tr B: 0.0495, Tr K: 0.0456, Tr D: 0.0196, Te B: 0.0491, Te K: 0.0414, Te D: 0.0172, Trus: 0.1873\n",
      "E: 014, Tr B: 0.0494, Tr K: 0.0451, Tr D: 0.0196, Te B: 0.0489, Te K: 0.0409, Te D: 0.0174, Trus: 0.1945\n",
      "E: 015, Tr B: 0.0493, Tr K: 0.0448, Tr D: 0.0199, Te B: 0.0490, Te K: 0.0408, Te D: 0.0178, Trus: 0.1872\n",
      "E: 016, Tr B: 0.0493, Tr K: 0.0446, Tr D: 0.0193, Te B: 0.0486, Te K: 0.0406, Te D: 0.0172, Trus: 0.1888\n",
      "E: 017, Tr B: 0.0494, Tr K: 0.0446, Tr D: 0.0195, Te B: 0.0489, Te K: 0.0408, Te D: 0.0175, Trus: 0.1879\n",
      "E: 018, Tr B: 0.0493, Tr K: 0.0443, Tr D: 0.0195, Te B: 0.0486, Te K: 0.0404, Te D: 0.0173, Trus: 0.1855\n",
      "E: 019, Tr B: 0.0493, Tr K: 0.0445, Tr D: 0.0196, Te B: 0.0489, Te K: 0.0407, Te D: 0.0175, Trus: 0.1811\n",
      "E: 020, Tr B: 0.0494, Tr K: 0.0443, Tr D: 0.0196, Te B: 0.0486, Te K: 0.0406, Te D: 0.0176, Trus: 0.1905\n",
      "E: 021, Tr B: 0.0495, Tr K: 0.0443, Tr D: 0.0195, Te B: 0.0489, Te K: 0.0406, Te D: 0.0174, Trus: 0.1883\n",
      "E: 022, Tr B: 0.0495, Tr K: 0.0441, Tr D: 0.0197, Te B: 0.0489, Te K: 0.0405, Te D: 0.0177, Trus: 0.1905\n",
      "E: 023, Tr B: 0.0495, Tr K: 0.0440, Tr D: 0.0196, Te B: 0.0486, Te K: 0.0405, Te D: 0.0175, Trus: 0.1897\n",
      "E: 024, Tr B: 0.0496, Tr K: 0.0440, Tr D: 0.0197, Te B: 0.0487, Te K: 0.0407, Te D: 0.0178, Trus: 0.1848\n",
      "E: 025, Tr B: 0.0494, Tr K: 0.0440, Tr D: 0.0199, Te B: 0.0488, Te K: 0.0406, Te D: 0.0180, Trus: 0.1852\n",
      "E: 026, Tr B: 0.0496, Tr K: 0.0439, Tr D: 0.0195, Te B: 0.0488, Te K: 0.0404, Te D: 0.0175, Trus: 0.1847\n",
      "E: 027, Tr B: 0.0495, Tr K: 0.0435, Tr D: 0.0197, Te B: 0.0488, Te K: 0.0401, Te D: 0.0176, Trus: 0.1867\n",
      "E: 028, Tr B: 0.0496, Tr K: 0.0439, Tr D: 0.0199, Te B: 0.0487, Te K: 0.0406, Te D: 0.0179, Trus: 0.1811\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-25a735b6dd71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_mean_biological_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_kabsch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mean_distance_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_true_hics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_pred_structures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_true_structures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred_distances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrain_true_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-73f5351cb196>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpred_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhic_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mpred_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ce452054c8c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, is_training)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNOISE_VARIANCE\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_BINS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_BINS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_encoder_layer_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_biological_losses_all_epochs = []\n",
    "train_kabsch_losses_all_epochs = []\n",
    "train_distance_losses_all_epochs = []\n",
    "\n",
    "test_biological_losses_all_epochs = []\n",
    "test_kabsch_losses_all_epochs = []\n",
    "test_distance_losses_all_epochs = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "trussart_test_kabsch_losses_all_epochs = []\n",
    "\n",
    "for epoch in range(1, NB_EPOCHS+1):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    \n",
    "    ### Training\n",
    "    train_mean_biological_loss, train_mean_kabsch_loss, train_mean_distance_loss, train_true_hics, \\\n",
    "        train_pred_structures, train_true_structures, train_pred_distances, \\\n",
    "            train_true_distances = evaluate(train_loader) \n",
    "    \n",
    "    # Store results\n",
    "    train_biological_losses_all_epochs.append(train_mean_biological_loss)\n",
    "    train_kabsch_losses_all_epochs.append(train_mean_kabsch_loss)    \n",
    "    train_distance_losses_all_epochs.append(train_mean_distance_loss)\n",
    "    \n",
    "    ### Testing\n",
    "    test_mean_biological_loss, test_mean_kabsch_loss, test_mean_distance_loss, test_true_hics, \\\n",
    "        test_pred_structures, test_true_structures, test_pred_distances, \\\n",
    "            test_true_distances = evaluate(test_loader) \n",
    "    \n",
    "    ### Trussart test\n",
    "    trussart_test_kabsch_loss = compute_trussart_test_kabsch_loss()\n",
    "    \n",
    "    # Store results\n",
    "    test_biological_losses_all_epochs.append(test_mean_biological_loss)\n",
    "    test_kabsch_losses_all_epochs.append(test_mean_kabsch_loss)    \n",
    "    test_distance_losses_all_epochs.append(test_mean_distance_loss)\n",
    "    \n",
    "    trussart_test_kabsch_losses_all_epochs.append(trussart_test_kabsch_loss)\n",
    "    \n",
    "    print('E: {:03d}, Tr B: {:.4f}, Tr K: {:.4f}, Tr D: {:.4f}, Te B: {:.4f}, Te K: {:.4f}, Te D: {:.4f}, Trus: {:.4f}'.format(\\\n",
    "        epoch, train_mean_biological_loss, train_mean_kabsch_loss, train_mean_distance_loss, \\\n",
    "            test_mean_biological_loss, test_mean_kabsch_loss, test_mean_distance_loss, trussart_test_kabsch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_biological_losses_all_epochs, label='Train Bio')\n",
    "plt.plot(test_biological_losses_all_epochs, label='Test Bio')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_kabsch_losses_all_epochs, label='Train Kabsch')\n",
    "plt.plot(test_kabsch_losses_all_epochs, label='Test Kabsch')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0b665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_distance_losses_all_epochs, label='Train Dist')\n",
    "plt.plot(test_distance_losses_all_epochs, label='Test Dist')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trussart_test_kabsch_losses_all_epochs, label='Test Trussart Kabsch')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52a3fb",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_TESTED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f29f0",
   "metadata": {},
   "source": [
    "### Test distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55267a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ground_truth_matrix = test_true_distances[GRAPH_TESTED*NB_BINS:GRAPH_TESTED*NB_BINS+NB_BINS, :]\n",
    "axes[0].imshow(ground_truth_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "reconstruction_matrix = test_pred_distances[GRAPH_TESTED*NB_BINS:GRAPH_TESTED*NB_BINS+NB_BINS, :]\n",
    "axes[1].imshow(reconstruction_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89fbc7",
   "metadata": {},
   "source": [
    "### Test latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 50))\n",
    "\n",
    "test_true_structure = test_true_structures[GRAPH_TESTED]\n",
    "test_pred_structure = test_pred_structures[GRAPH_TESTED]\n",
    "\n",
    "test_pred_structure_superposed, test_true_structure_superposed = \\\n",
    "        kabsch_superimposition_numpy(test_pred_structure, test_true_structure)\n",
    "\n",
    "x_pred = test_pred_structure_superposed[:, 0]  \n",
    "y_pred = test_pred_structure_superposed[:, 1]\n",
    "z_pred = test_pred_structure_superposed[:, 2]\n",
    "\n",
    "x_true = test_true_structure_superposed[:, 0]  \n",
    "y_true = test_true_structure_superposed[:, 1]\n",
    "z_true = test_true_structure_superposed[:, 2]\n",
    "\n",
    "# Initialize figure with 4 3D subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "\n",
    "# adding surfaces to subplots.\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "    x=x_true, y=y_true, z=z_true,\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=np.asarray(range(len(x_true))),\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    line=dict(\n",
    "        color='darkblue',\n",
    "        width=2\n",
    "    )\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "    x=x_pred, y=y_pred, z=z_pred,\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=np.asarray(range(len(x_pred))),\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    line=dict(\n",
    "        color='darkblue',\n",
    "        width=2\n",
    "    )\n",
    "),row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Shape comparison\n",
    "print('Kabsch distance is ' + str(kabsch_distance_numpy(test_pred_structure, test_true_structure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "kabsch_distances = []\n",
    "\n",
    "for graph_index in range(test_size):\n",
    "\n",
    "    test_true_structure = test_true_structures[graph_index,:,:]\n",
    "    test_pred_structure = test_pred_structures[graph_index,:,:]\n",
    "    \n",
    "    d = kabsch_distance_numpy(test_pred_structure, test_true_structure)\n",
    "    kabsch_distances.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ee731",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(kabsch_distances, 100, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('mean: ' + str(np.mean(kabsch_distances)))\n",
    "print('median: ' + str(np.median(kabsch_distances)))\n",
    "print('variance: ' + str(np.var(kabsch_distances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28100067",
   "metadata": {},
   "source": [
    "# Save test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ROOT = '../../../../results/non_ae/synthetic_biological/linear/'\n",
    "LAMBDA_CONFIGURATION = str(LAMBDA_BIOLOGICAL) + '_' + str(LAMBDA_KABSCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_losses_' + LAMBDA_CONFIGURATION + '.txt', losses)\n",
    "\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_train_biological_losses_all_epochs_' +\n",
    "               LAMBDA_CONFIGURATION + '.txt', train_biological_losses_all_epochs)\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_test_biological_losses_all_epochs_' +\n",
    "               LAMBDA_CONFIGURATION + '.txt', test_biological_losses_all_epochs)\n",
    "\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_train_kabsch_losses_all_epochs_' + \n",
    "           LAMBDA_CONFIGURATION + '.txt', train_kabsch_losses_all_epochs)\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_test_kabsch_losses_all_epochs_' + \n",
    "           LAMBDA_CONFIGURATION + '.txt', test_kabsch_losses_all_epochs)\n",
    "\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_train_distance_losses_all_epochs_' +\n",
    "           LAMBDA_CONFIGURATION + '.txt', train_distance_losses_all_epochs)\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_test_distance_losses_all_epochs_' + \n",
    "           LAMBDA_CONFIGURATION + '.txt', test_distance_losses_all_epochs)\n",
    "\n",
    "np.savetxt(RESULTS_ROOT + 'non_ae_synthetic_biological_linear_trussart_test_kabsch_losses_all_epochs_' +\n",
    "               LAMBDA_CONFIGURATION + '.txt', trussart_test_kabsch_losses_all_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24e4a5",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \n",
    "           '../../../../models/non_ae/synthetic_biological/linear/non_ae_synthetic_biological_linear_model_' + \n",
    "           LAMBDA_CONFIGURATION + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

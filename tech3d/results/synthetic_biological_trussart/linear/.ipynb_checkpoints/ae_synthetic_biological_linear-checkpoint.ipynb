{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educated-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File processing\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data display \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import linalg as LAtorch\n",
    "from numpy import linalg as LAnumpy\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch_geometric.data import Data, InMemoryDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac13c5",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465431e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 200\n",
    "BATCH_SIZE = 10\n",
    "NB_NODES = 202\n",
    "EMBEDDING_SIZE = 3 # Euclidean 3D space\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 0\n",
    "LAMBDA_STRUCTURE = 10\n",
    "LAMBDA_DISTANCE = 1\n",
    "TRAIN_DATASET_SIZE = 800\n",
    "TEST_DATASET_SIZE = 200\n",
    "NOISE_VARIANCE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc7277",
   "metadata": {},
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26605809",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-gnome",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab last 4 digits of the file txt name:\n",
    "def last_4digits(x):\n",
    "    return(x[-8:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412f446",
   "metadata": {},
   "source": [
    "### Hic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_hics = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/ae/synthetic_biological/train/hic_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_hic = np.loadtxt('../../../../data/ae/synthetic_biological/train/hic_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_hics.append(current_train_transfer_learning_hic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_hics = []\n",
    "\n",
    "file_list = os.listdir('../../../../data/synthetic_biological/test/hic_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_hic = np.loadtxt('../../../../data/synthetic_tad/test/hic_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_hics.append(current_test_transfer_learning_hic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b876b",
   "metadata": {},
   "source": [
    "### Structure matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eefd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_structures = []\n",
    "\n",
    "file_list = os.listdir('../../../data/synthetic_tad/train/structure_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_structure = \\\n",
    "        np.loadtxt('../../../data/synthetic_tad/train/structure_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_structures.append(current_train_transfer_learning_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a41608",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_structures = []\n",
    "\n",
    "file_list = os.listdir('../../../data/synthetic_tad/test/structure_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_structure = \\\n",
    "        np.loadtxt('../../../data/synthetic_tad/test/structure_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_structures.append(current_test_transfer_learning_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24328a71",
   "metadata": {},
   "source": [
    "### Distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfer_learning_distances = []\n",
    "\n",
    "file_list = os.listdir('../../../data/synthetic_tad/train/distance_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_train_transfer_learning_distance = \\\n",
    "            np.loadtxt('../../../data/synthetic_tad/train/distance_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    train_transfer_learning_distances.append(current_train_transfer_learning_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfer_learning_distances = []\n",
    "\n",
    "file_list = os.listdir('../../../data/synthetic_tad/test/distance_matrices/')\n",
    "\n",
    "for file_name in sorted(filter(lambda x: x.endswith('.txt'), file_list), key = last_4digits):\n",
    "    current_test_transfer_learning_distance = \\\n",
    "            np.loadtxt('../../../data/synthetic_tad/test/distance_matrices/'\\\n",
    "                                                     + file_name, dtype='f', delimiter=' ')\n",
    "    test_transfer_learning_distances.append(current_test_transfer_learning_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0a949",
   "metadata": {},
   "source": [
    "### Final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cfddc",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(VanillaDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if is_training:\n",
    "            return ['processed_transfer_learning_train_synthetic_data.txt']\n",
    "        else:\n",
    "            return ['processed_transfer_learning_test_synthetic_data.txt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        if is_training:\n",
    "            dataset_size = TRAIN_DATASET_SIZE\n",
    "        else:\n",
    "            dataset_size = TEST_DATASET_SIZE\n",
    "        \n",
    "        for i in tqdm(range(dataset_size)):\n",
    "            \n",
    "            if is_training:\n",
    "                transfer_learning_hic = train_transfer_learning_hics[i]\n",
    "                transfer_learning_structure = train_transfer_learning_structures[i]\n",
    "                transfer_learning_distance_matrix = train_transfer_learning_distances[i]\n",
    "            else:\n",
    "                transfer_learning_hic = test_transfer_learning_hics[i]\n",
    "                transfer_learning_structure = test_transfer_learning_structures[i]\n",
    "                transfer_learning_distance_matrix = test_transfer_learning_distances[i]\n",
    "               \n",
    "            hic_matrix = torch.FloatTensor(transfer_learning_hic)\n",
    "            structure_matrix = torch.FloatTensor(transfer_learning_structure)\n",
    "            distance_matrix = torch.FloatTensor(transfer_learning_distance_matrix)\n",
    "\n",
    "            data = Data(hic_matrix=hic_matrix, structure_matrix=structure_matrix, distance_matrix=distance_matrix)\n",
    "            data_list.append(data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VanillaDataset('../')\n",
    "train_dataset = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_dataset)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6adbb",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45875f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd917ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VanillaDataset('../')\n",
    "test_dataset = test_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6319b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(test_dataset)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede05c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863a3b1",
   "metadata": {},
   "source": [
    "# Synthetic Network: Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear_encoder_layer_1 = torch.nn.Linear(NB_NODES, 100)\n",
    "        self.linear_encoder_layer_2 = torch.nn.Linear(100, 50)\n",
    "        self.linear_encoder_layer_3 = torch.nn.Linear(50, EMBEDDING_SIZE)\n",
    "        \n",
    "        self.linear_decoder_layer_1 = torch.nn.Linear(NB_NODES, NB_NODES)\n",
    "        \n",
    "        self.diagonal_mask = torch.eye(NB_NODES, dtype=torch.float).repeat(BATCH_SIZE, 1, 1)\n",
    "        self.zero_output = torch.zeros(NB_NODES, NB_NODES, dtype=torch.float).repeat(BATCH_SIZE, 1, 1)\n",
    "        \n",
    "    def forward(self, x, is_training=False):\n",
    "        \n",
    "        x = torch.reshape(x, (BATCH_SIZE, NB_NODES, NB_NODES))\n",
    "        if is_training:\n",
    "            x = x + (NOISE_VARIANCE**0.5)*torch.randn(BATCH_SIZE, NB_NODES, NB_NODES)\n",
    "        \n",
    "        z = self.linear_encoder_layer_1(x)\n",
    "        z = F.relu(z)\n",
    "        z = self.linear_encoder_layer_2(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.linear_encoder_layer_3(z)\n",
    "        z = F.relu(z)\n",
    "        z = centralize_and_normalize_torch(z)\n",
    "        \n",
    "        w = torch.cdist(z, z, p=2)\n",
    "        \n",
    "        y = self.linear_decoder_layer_1(w)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        # Set y to be symmetric\n",
    "        y = (y + torch.transpose(y, 1, 2))/2\n",
    "        \n",
    "        # Clamp ouput between 0 and 1 (Note: need to use Min Max scaler for input)\n",
    "        y = torch.clamp(y, min=0, max=1)\n",
    "        \n",
    "        # Set diagonal prediction HiC frequencies to 0\n",
    "        y = torch.where(self.diagonal_mask > 0.0, self.zero_output, y)\n",
    "\n",
    "        return y, z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70de68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "synthetic_model = torch.load('../../../models/synthetic/linear/synthetic_linear_model.pt').to(device)\n",
    "\n",
    "# Freez training of weights\n",
    "for param in synthetic_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-invite",
   "metadata": {},
   "source": [
    "# Biological Network: Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear_encoder_layer_x = torch.nn.Linear(NB_NODES, NB_NODES)\n",
    "        \n",
    "        self.linear_decoder_layer_y = torch.nn.Linear(NB_NODES, NB_NODES)\n",
    "        self.linear_decoder_layer_z = torch.nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.linear_decoder_layer_w = torch.nn.Linear(NB_NODES, NB_NODES)\n",
    "        \n",
    "        self.diagonal_mask = torch.eye(NB_NODES, dtype=torch.float).repeat(BATCH_SIZE, 1, 1)\n",
    "        self.zero_output = torch.zeros(NB_NODES, NB_NODES, dtype=torch.float).repeat(BATCH_SIZE, 1, 1)\n",
    "        \n",
    "    def forward(self, x, is_training=False):\n",
    "        \n",
    "        x = torch.reshape(x, (BATCH_SIZE, NB_NODES, NB_NODES))\n",
    "        if is_training:\n",
    "            x = x + (NOISE_VARIANCE**0.5)*torch.randn(BATCH_SIZE, NB_NODES, NB_NODES)\n",
    "        \n",
    "        x = self.linear_encoder_layer_x(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        y, z, w = synthetic_model(x)\n",
    "        \n",
    "        # Deal with y\n",
    "        y = self.linear_decoder_layer_y(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        # Set y to be symmetric\n",
    "        y = (y + torch.transpose(y, 1, 2))/2\n",
    "        \n",
    "        # Clamp ouput between 0 and 1 (Note: need to use Min Max scaler for input)\n",
    "        y = torch.clamp(y, min=0, max=1)\n",
    "        \n",
    "        # Set diagonal prediction HiC frequencies to 0\n",
    "        y = torch.where(self.diagonal_mask > 0.0, self.zero_output, y)\n",
    "        \n",
    "        # Deal with z\n",
    "        z = self.linear_decoder_layer_z(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        # Deal with w\n",
    "        w = self.linear_decoder_layer_w(w)\n",
    "        w = F.relu(w)\n",
    "        \n",
    "        return y, z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0534e9",
   "metadata": {},
   "source": [
    "# Procruste analysis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54443bd5",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_torch(z):\n",
    "    return z - torch.repeat_interleave(torch.reshape(torch.mean(z, axis=1), (-1,1,EMBEDDING_SIZE)), NB_NODES, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_torch(z):\n",
    "    \n",
    "    norms = LAtorch.norm(z, 2, dim=2)\n",
    "    max_norms, _ = torch.max(norms, axis=1)\n",
    "    max_norms = torch.reshape(max_norms, (BATCH_SIZE,1,1))\n",
    "    max_norms = torch.repeat_interleave(max_norms, NB_NODES, dim=1)\n",
    "    max_norms = torch.repeat_interleave(max_norms, EMBEDDING_SIZE, dim=2)\n",
    "    max_norms[max_norms == 0] = 1\n",
    "    \n",
    "    return z / max_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_and_normalize_torch(z):\n",
    "    \n",
    "    # Translate\n",
    "    z = centralize_torch(z)\n",
    "    \n",
    "    # Scale\n",
    "    z = normalize_torch(z)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_loss_fct(pred_structure, true_structure):\n",
    "    \n",
    "    # Rotation (solution for the constrained orthogonal Procrustes problem, subject to det(R) = 1)\n",
    "    m = torch.matmul(true_structure, torch.transpose(pred_structure, 1, 2))    \n",
    "    u, s, vh = torch.svd_lowrank(m, q=EMBEDDING_SIZE)\n",
    "    r = torch.matmul(u, torch.transpose(vh, 1, 2))\n",
    "\n",
    "    pred_structure = torch.matmul(r, pred_structure)\n",
    "    \n",
    "    return torch.mean(torch.sum(torch.square(pred_structure - true_structure), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5c470",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30307ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_numpy(z):\n",
    "    return z - np.mean(z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numpy(z):\n",
    "    \n",
    "    norm = LAnumpy.norm(z, 2, axis=1)\n",
    "    max_norm = np.max(norm, axis=0)\n",
    "    if max_norm == 0:\n",
    "        max_norm = 1\n",
    "    \n",
    "    return z / max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f22cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_and_normalize_numpy(z):\n",
    "    \n",
    "    # Translate\n",
    "    z = centralize_numpy(z)\n",
    "    \n",
    "    # Scale\n",
    "    z = normalize_numpy(z)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_superimposition_numpy(pred_structure, true_structure):\n",
    "    \n",
    "    # Centralize and normalize to unit ball\n",
    "    pred_structure_unit_ball = centralize_and_normalize_numpy(pred_structure)\n",
    "    true_structure_unit_ball = centralize_and_normalize_numpy(true_structure)\n",
    "    \n",
    "    # Rotation (solution for the constrained orthogonal Procrustes problem, subject to det(R) = 1)\n",
    "    m = np.matmul(true_structure_unit_ball, np.transpose(pred_structure_unit_ball))\n",
    "    u, s, vh = np.linalg.svd(m, full_matrices=False)\n",
    "    r = np.matmul(u, vh)\n",
    "    pred_structure_unit_ball = np.matmul(r, pred_structure_unit_ball)\n",
    "    \n",
    "    return pred_structure_unit_ball, true_structure_unit_ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18470850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_distance_numpy(pred_structure, true_structure):\n",
    "    \n",
    "    # Centralize and normalize to unit ball\n",
    "    pred_structure_unit_ball = centralize_and_normalize_numpy(pred_structure)\n",
    "    true_structure_unit_ball = centralize_and_normalize_numpy(true_structure)\n",
    "    \n",
    "    # Rotation (solution for the constrained orthogonal Procrustes problem, subject to det(R) = 1)\n",
    "    m = np.matmul(true_structure_unit_ball, np.transpose(pred_structure_unit_ball))\n",
    "    u, s, vh = np.linalg.svd(m, full_matrices=False)\n",
    "    r = np.matmul(u, vh)\n",
    "    pred_structure_unit_ball = np.matmul(r, pred_structure_unit_ball)\n",
    "    \n",
    "    # Structure comparison\n",
    "    d = np.mean(np.sum(np.square(pred_structure_unit_ball - true_structure_unit_ball), axis=1))\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-prerequisite",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "hic_loss_fct = torch.nn.MSELoss()\n",
    "distance_loss_fct = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b37ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([matplotlib.lines.Line2D([0], [0], color=\"c\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"b\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"k\", lw=4)], \n",
    "               ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_hic, pred_structure, pred_distance = model(data.hic_matrix, is_training=True)\n",
    "        \n",
    "        pred_hic = torch.reshape(pred_hic, (BATCH_SIZE*NB_NODES, NB_NODES))\n",
    "        true_hic = data.hic_matrix.to(device)\n",
    "        \n",
    "        true_structure = data.structure_matrix.to(device)\n",
    "        true_structure = torch.reshape(true_structure, (BATCH_SIZE, NB_NODES, EMBEDDING_SIZE))\n",
    "        \n",
    "        pred_distance = torch.reshape(pred_distance, (BATCH_SIZE*NB_NODES, NB_NODES))\n",
    "        true_distance = data.distance_matrix.to(device)\n",
    "        \n",
    "        # HiC loss\n",
    "        hic_loss = hic_loss_fct(pred_hic, true_hic)\n",
    "        \n",
    "        # Structure loss\n",
    "        structure_loss = structure_loss_fct(pred_structure, true_structure)\n",
    "        \n",
    "        # Distance loss \n",
    "        distance_loss = distance_loss_fct(pred_distance, true_distance)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = hic_loss + LAMBDA_STRUCTURE * structure_loss + LAMBDA_DISTANCE * distance_loss\n",
    "        \n",
    "#         with torch.autograd.detect_anomaly():\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        \n",
    "        # Plot grad flow\n",
    "#         plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred_hics = []\n",
    "    true_hics = []\n",
    "    \n",
    "    pred_structures = []\n",
    "    true_structures = []\n",
    "    \n",
    "    pred_distances = []\n",
    "    true_distances = []\n",
    "    \n",
    "    hic_losses = []\n",
    "    structure_losses = []\n",
    "    distance_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            \n",
    "            pred_hic, pred_structure, pred_distance = model(data.hic_matrix)\n",
    "            \n",
    "            pred_hic = pred_hic.detach().cpu()\n",
    "            pred_structure = pred_structure.detach().cpu()\n",
    "            pred_distance = pred_distance.detach().cpu()\n",
    "            \n",
    "            pred_hic = torch.reshape(pred_hic, (BATCH_SIZE*NB_NODES, NB_NODES))\n",
    "            pred_distance = torch.reshape(pred_distance, (BATCH_SIZE*NB_NODES, NB_NODES))\n",
    "            \n",
    "            true_hic = data.hic_matrix.detach().cpu()\n",
    "            true_structure = data.structure_matrix.detach().cpu()\n",
    "            true_distance = data.distance_matrix.detach().cpu()\n",
    "            \n",
    "            true_structure = torch.reshape(true_structure, (BATCH_SIZE, NB_NODES, EMBEDDING_SIZE))\n",
    "            \n",
    "            # Hic loss\n",
    "            hic_loss = hic_loss_fct(pred_hic, true_hic).numpy()\n",
    "            hic_losses.append(hic_loss)\n",
    "        \n",
    "            # Structure loss\n",
    "            structure_loss = structure_loss_fct(pred_structure, true_structure).numpy()\n",
    "            structure_losses.append(structure_loss)\n",
    "            \n",
    "            # Distance \n",
    "            distance_loss = distance_loss_fct(pred_distance, true_distance).numpy()\n",
    "            distance_losses.append(distance_loss)\n",
    "            \n",
    "            # To numpy\n",
    "            pred_hic = pred_hic.numpy()\n",
    "            true_hic = true_hic.numpy()\n",
    "            \n",
    "            pred_structure = pred_structure.numpy()\n",
    "            true_structure = true_structure.numpy()\n",
    "            \n",
    "            pred_distance = pred_distance.numpy()\n",
    "            true_distance = true_distance.numpy()\n",
    "            \n",
    "            # Store results\n",
    "            pred_hics.append(pred_hic)\n",
    "            true_hics.append(true_hic)\n",
    "            \n",
    "            pred_structures.append(pred_structure)\n",
    "            true_structures.append(true_structure)\n",
    "            \n",
    "            pred_distances.append(pred_distance)\n",
    "            true_distances.append(true_distance)\n",
    "    \n",
    "    # Format reesults\n",
    "    pred_hics = np.vstack(pred_hics)\n",
    "    true_hics = np.vstack(true_hics)\n",
    "    \n",
    "    pred_structures = np.vstack(pred_structures)\n",
    "    true_structures = np.vstack(true_structures)\n",
    "    \n",
    "    pred_distances = np.vstack(pred_distances)\n",
    "    true_distances = np.vstack(true_distances)\n",
    "    \n",
    "    # Compute mean losses\n",
    "    mean_hic_loss = np.mean(np.asarray(hic_losses).flatten())\n",
    "    mean_structure_loss = np.mean(np.asarray(structure_losses).flatten())\n",
    "    mean_distance_loss = np.mean(np.asarray(distance_losses).flatten())\n",
    "    \n",
    "    \n",
    "    return mean_hic_loss, mean_structure_loss, mean_distance_loss, pred_hics, true_hics, \\\n",
    "            pred_structures, true_structures, pred_distances, true_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hic_losses_all_epochs = []\n",
    "train_structure_losses_all_epochs = []\n",
    "train_distance_losses_all_epochs = []\n",
    "\n",
    "test_hic_losses_all_epochs = []\n",
    "test_structure_losses_all_epochs = []\n",
    "test_distance_losses_all_epochs = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NB_EPOCHS+1):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    \n",
    "    ### Training\n",
    "    train_mean_hic_loss, train_mean_structure_loss, train_mean_distance_loss, train_pred_hics, train_true_hics, \\\n",
    "        train_pred_structures, train_true_structures, train_pred_distances, \\\n",
    "            train_true_distances = evaluate(train_loader) \n",
    "    \n",
    "    # Store results\n",
    "    train_hic_losses_all_epochs.append(train_mean_hic_loss)\n",
    "    train_structure_losses_all_epochs.append(train_mean_structure_loss)    \n",
    "    train_distance_losses_all_epochs.append(train_mean_distance_loss)\n",
    "    \n",
    "    ### Testing\n",
    "    test_mean_hic_loss, test_mean_structure_loss, test_mean_distance_loss, test_pred_hics, test_true_hics, \\\n",
    "        test_pred_structures, test_true_structures, test_pred_distances, \\\n",
    "            test_true_distances = evaluate(test_loader) \n",
    "    \n",
    "    # Store results\n",
    "    test_hic_losses_all_epochs.append(test_mean_hic_loss)\n",
    "    test_structure_losses_all_epochs.append(test_mean_structure_loss)    \n",
    "    test_distance_losses_all_epochs.append(test_mean_distance_loss)\n",
    "    \n",
    "    print('E: {:03d}, Tr H: {:.4f}, Tr S: {:.4f}, Tr D: {:.4f}, Te H: {:.4f}, Te S: {:.4f}, Te D: {:.4f}'.format(\\\n",
    "        epoch, train_mean_hic_loss, train_mean_structure_loss, train_mean_distance_loss, \\\n",
    "               test_mean_hic_loss, test_mean_structure_loss, test_mean_distance_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_hic_losses_all_epochs, label='Train Hic')\n",
    "plt.plot(test_hic_losses_all_epochs, label='Test Hic')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_structure_losses_all_epochs, label='Train Struct')\n",
    "plt.plot(test_structure_losses_all_epochs, label='Test Struct')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0b665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_distance_losses_all_epochs, label='Train Dist')\n",
    "plt.plot(test_distance_losses_all_epochs, label='Test Dist')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52a3fb",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_TESTED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9dcdf",
   "metadata": {},
   "source": [
    "### Test reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ground_truth_matrix = test_true_hics[GRAPH_TESTED*NB_NODES:GRAPH_TESTED*NB_NODES+NB_NODES, :]\n",
    "axes[0].imshow(ground_truth_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "reconstruction_matrix = test_pred_hics[GRAPH_TESTED*NB_NODES:GRAPH_TESTED*NB_NODES+NB_NODES, :]\n",
    "axes[1].imshow(reconstruction_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f29f0",
   "metadata": {},
   "source": [
    "### Test distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55267a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ground_truth_matrix = test_true_distances[GRAPH_TESTED*NB_NODES:GRAPH_TESTED*NB_NODES+NB_NODES, :]\n",
    "axes[0].imshow(ground_truth_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "reconstruction_matrix = test_pred_distances[GRAPH_TESTED*NB_NODES:GRAPH_TESTED*NB_NODES+NB_NODES, :]\n",
    "axes[1].imshow(reconstruction_matrix, cmap='hot', interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89fbc7",
   "metadata": {},
   "source": [
    "### Test latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 50))\n",
    "\n",
    "test_true_structure = test_true_structures[GRAPH_TESTED]\n",
    "test_pred_structure = test_pred_structures[GRAPH_TESTED]\n",
    "\n",
    "test_pred_structure_pro, test_true_structure_pro = \\\n",
    "        procrustes_superimposition_numpy(test_pred_structure, test_true_structure)\n",
    "\n",
    "x_pred = test_pred_structure_pro[:, 0]  \n",
    "y_pred = test_pred_structure_pro[:, 1]\n",
    "z_pred = test_pred_structure_pro[:, 2]\n",
    "\n",
    "x_true = test_true_structure_pro[:, 0]  \n",
    "y_true = test_true_structure_pro[:, 1]\n",
    "z_true = test_true_structure_pro[:, 2]\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot3D(x_true, y_true, z_true, c='blue', alpha=0.5, lw=4.5)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot3D(x_pred, y_pred, z_pred, c='blue', alpha=0.5, lw=4.5)\n",
    "\n",
    "# Shape comparison\n",
    "d = np.mean(np.sum(np.square(test_pred_structure_pro - test_true_structure_pro), axis=1))\n",
    "print('Procrustes distance is ' + str(d))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "procruste_distances = []\n",
    "\n",
    "for graph_index in range(test_size):\n",
    "\n",
    "    test_true_structure = test_true_structures[graph_index,:,:]\n",
    "    test_pred_structure = test_pred_structures[graph_index,:,:]\n",
    "    \n",
    "    d = procrustes_distance_numpy(test_pred_structure, test_true_structure)\n",
    "    procruste_distances.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ee731",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(procruste_distances, 100, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('mean: ' + str(np.mean(procruste_distances)))\n",
    "print('median: ' + str(np.median(procruste_distances)))\n",
    "print('variance: ' + str(np.var(procruste_distances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24e4a5",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../../../models/synthetic_tad_extended/linear/synthetic_tad_extended_linear_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9e264",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ccf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../../results/synthetic_tad_extended/linear/synthetic_tad_extended_linear_structure_train_losses.txt', \n",
    "           train_structure_losses_all_epochs, delimiter=',')\n",
    "np.savetxt('../../../results/synthetic_tad_extended/linear/synthetic_tad_extended_linear_structure_test_losses.txt', \n",
    "           test_structure_losses_all_epochs, delimiter=',')\n",
    "np.savetxt('../../../results/synthetic_tad_extended/linear/synthetic_tad_extended_linear_structure_procruste_distances.txt', \n",
    "           procruste_distances, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
